# flexible-LLM-sharding
This repository is designed for individuals who want to run LLM locally with small vRAM (GPU memory) and RAM. It facilitates the efficient execution of unquantized LLM's (such as Llama2-70B) with vRAM&ge;6GB and RAM&ge;8GB, i.e. when standard offloading does not work.
